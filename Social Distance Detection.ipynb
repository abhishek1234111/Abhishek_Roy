{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import imutils\n",
    "import tensorflow\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "from scipy.spatial import distance as dist\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the paths\n",
    "LabelPath=r\"C:\\Users\\HP\\Desktop\\yolo-coco\\coco.names.txt\"\n",
    "WeightsPath=r\"C:\\Users\\HP\\Desktop\\yolo-coco\\yolov3.weights\"\n",
    "ConfigPath=r\"C:\\Users\\HP\\Desktop\\yolo-coco\\yolov3.cfg.txt\"\n",
    "MIN_CONF=0.3\n",
    "NMS_THRESH=0.3\n",
    "MIN_DISTANCE=50\n",
    "INPUT_VIDEO=r\"F:\\OBJECT DETECTION\\background video _ people _ walking _.mp4\"\n",
    "OUTPUT_VIDEO=r\"F:\\OBJECT DETECTION\\Social distance folder/Output.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single function definition for detecting people \n",
    "def detect_people(frame,net,ln,personIdx=0):\n",
    "    (h,w)=frame.shape[:2]\n",
    "    results=[]\n",
    "    \n",
    "    # construct a blob from the input frame and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes\n",
    "    # and associated probabilities\n",
    "    \n",
    "    blob=cv2.dnn.blobFromImage(frame,1/255.0,(416,416),swapRB=True,crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs=net.forward(ln)\n",
    "    \n",
    "    \n",
    "    # initialize our lists of detected bounding boxes, centroids, and\n",
    "    # confidences, respectively\n",
    "    boxes=[]\n",
    "    centroids=[]\n",
    "    confidences=[]\n",
    "    \n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores=detection[5:]\n",
    "            classid=np.argmax(scores)\n",
    "            confidence=scores[classid]\n",
    "            \n",
    "            if classid==personIdx and confidence>MIN_CONF:\n",
    "                box=detection[0:4]*np.array([w,h,w,h])\n",
    "                (centreX,centreY,width,height)=box.astype('int')\n",
    "                \n",
    "                x=int(centreX-(width/2))\n",
    "                y=int(centreY-(height/2))\n",
    "                \n",
    "                boxes.append([x,y,int(width),int(height)])\n",
    "                centroids.append((centreX,centreY))\n",
    "                confidences.append(float(confidence))\n",
    "                \n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "    # bounding boxes\n",
    "    idx=cv2.dnn.NMSBoxes(boxes,confidences,MIN_CONF,NMS_THRESH)\n",
    "    \n",
    "    if len(idx)>0:\n",
    "        for i in idx.flatten():\n",
    "            (x,y)=(boxes[i][0],boxes[i][1])\n",
    "            (w,h)=(boxes[i][2],boxes[i][3])\n",
    "            \n",
    "            r=(confidences[i],(x,y,x+w,y+h),centroids[i])\n",
    "            results.append(r)\n",
    "            \n",
    "    return results\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels=open(LabelPath).read().strip().split('\\n')\n",
    "args={'display':1}\n",
    "\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "net=cv2.dnn.readNetFromDarknet(ConfigPath,WeightsPath)\n",
    "\n",
    "ln=net.getLayerNames()\n",
    "ln=[ln[int(i)-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "\n",
    "# initialize the video stream and pointer to output video file\n",
    "vs=cv2.VideoCapture(INPUT_VIDEO if INPUT_VIDEO else 0)\n",
    "writer=None\n",
    "\n",
    "while True:\n",
    "    (grabbed,frame)=vs.read()\n",
    "    if not grabbed:\n",
    "        break\n",
    "    frame=imutils.resize(frame,width=700)\n",
    "    results=detect_people(frame,net,ln,personIdx=Labels.index('person'))\n",
    "    \n",
    "    violate=set()\n",
    "    \n",
    "    # ensure there are *at least* two people detections (required in\n",
    "    # order to compute our pairwise distance maps)\n",
    "    if len(results)>2:\n",
    "        centroid=np.array([r[2] for r in results])\n",
    "        D=dist.cdist(centroid,centroid,metric='euclidean')\n",
    "        \n",
    "        \n",
    "        # loop over the upper triangular of the distance matrix\n",
    "        for i in range(0,D.shape[0]):\n",
    "            for j in range(i+1,D.shape[1]):\n",
    "                \n",
    "                # check to see if the distance between any two\n",
    "                # centroid pairs is less than the configured number\n",
    "                # of pixels\n",
    "                \n",
    "                if D[i,j]<MIN_DISTANCE:\n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "                    \n",
    "    # loop over the results              \n",
    "    for(i,(probs,boxes,centroids)) in enumerate(results):\n",
    "        # extract the bounding box and centroid coordinates, then\n",
    "        # initialize the color of the annotation\n",
    "        \n",
    "        (startX,startY,endX,endY)=boxes\n",
    "        (cx,cy)=centroids\n",
    "        colors=(0,255,0)\n",
    "        \n",
    "        \n",
    "        # if the index pair exists within the violation set, then\n",
    "        # update the color\n",
    "        \n",
    "        if i in violate:\n",
    "            colors=(0,0,255)\n",
    "            \n",
    "            cv2.rectangle(frame,(startX,startY),(endX,endY),colors,2)\n",
    "            cv2.circle(frame,(cx,cy),5,colors,1)\n",
    "            \n",
    "        text='Social Distance Violated:{}'.format(len(violate))\n",
    "        cv2.putText(frame,text,(10,frame.shape[0]-25),cv2.FONT_HERSHEY_SIMPLEX,0.85,(0,0,255),2)\n",
    "        \n",
    "        if args['display']>0:\n",
    "            cv2.imshow('Output',frame)\n",
    "            key=cv2.waitKey(1) & 0xFF\n",
    "            if key==ord('q'):\n",
    "                break\n",
    "        if OUTPUT_VIDEO!='' and writer is None:\n",
    "            \n",
    "            # initialize our video writer\n",
    "            fourcc=cv2.VideoWriter_fourcc(*'MJPG')\n",
    "            writer=cv2.VideoWriter(OUTPUT_VIDEO,fourcc,25,(frame.shape[0],frame.shape[1]),True)\n",
    "            \n",
    "        \n",
    "        # if the video writer is not None, write the frame to the output\n",
    "        # video file\n",
    "        \n",
    "        if writer is not None:\n",
    "            writer.write(frame)\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
