{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow\n",
    "import keras\n",
    "import imutils\n",
    "from imutils import paths\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "from skimage import io\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Flatten,Dense,Dropout,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignNet:\n",
    "    def build(width,height,depth,classes):\n",
    "        model=Sequential()\n",
    "        inputShape=(height,width,depth)\n",
    "        chanDim=-1\n",
    "        \n",
    "        model.add(Conv2D(8,(5,5),activation='relu',padding='same',input_shape=inputShape))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(Conv2D(16,(3,3),activation='relu',padding='same'))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(16,(3,3),activation='relu',padding='same'))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(classes,activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(basePath,csvPath):\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    \n",
    "    rows=open(csvPath).read().strip().split('\\n')[1:]\n",
    "    random.shuffle(rows)\n",
    "    \n",
    "    for (i,row) in enumerate(rows):\n",
    "        if i>0 and i%1000==0:\n",
    "            print(\"[INFO] processed {} total images\".format(i))\n",
    "        (label,imagePath)=row.strip().split(',')[-2:]\n",
    "        imagePath=os.path.sep.join([basePath,imagePath])\n",
    "        image=io.imread(imagePath)\n",
    "        image=transform.resize(image,(32,32))\n",
    "        image=exposure.equalize_adapthist(image,clip_limit=0.1)\n",
    "        \n",
    "        data.append(image)\n",
    "        labels.append(label)\n",
    "        \n",
    "    data=np.array(data)\n",
    "    labels=np.array(labels)\n",
    "    \n",
    "    return (data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'dataset':r\"F:\\Computer Vision Datasets\\Traffic Sign Detection\\archive (7)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelNames=open('Meta.csv').read().strip().split('\\n')[1:]\n",
    "labelNames=[l.split(\",\")[1] for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 1000 total images\n",
      "[INFO] processed 2000 total images\n",
      "[INFO] processed 3000 total images\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 5000 total images\n",
      "[INFO] processed 6000 total images\n",
      "[INFO] processed 7000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 9000 total images\n",
      "[INFO] processed 10000 total images\n",
      "[INFO] processed 11000 total images\n",
      "[INFO] processed 12000 total images\n",
      "[INFO] processed 13000 total images\n",
      "[INFO] processed 14000 total images\n",
      "[INFO] processed 15000 total images\n",
      "[INFO] processed 16000 total images\n",
      "[INFO] processed 17000 total images\n",
      "[INFO] processed 18000 total images\n",
      "[INFO] processed 19000 total images\n",
      "[INFO] processed 20000 total images\n",
      "[INFO] processed 21000 total images\n",
      "[INFO] processed 22000 total images\n",
      "[INFO] processed 23000 total images\n",
      "[INFO] processed 24000 total images\n",
      "[INFO] processed 25000 total images\n",
      "[INFO] processed 26000 total images\n",
      "[INFO] processed 27000 total images\n",
      "[INFO] processed 28000 total images\n",
      "[INFO] processed 29000 total images\n",
      "[INFO] processed 30000 total images\n",
      "[INFO] processed 31000 total images\n",
      "[INFO] processed 32000 total images\n",
      "[INFO] processed 33000 total images\n",
      "[INFO] processed 34000 total images\n",
      "[INFO] processed 35000 total images\n",
      "[INFO] processed 36000 total images\n",
      "[INFO] processed 37000 total images\n",
      "[INFO] processed 38000 total images\n",
      "[INFO] processed 39000 total images\n",
      "[INFO] processed 1000 total images\n",
      "[INFO] processed 2000 total images\n",
      "[INFO] processed 3000 total images\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 5000 total images\n",
      "[INFO] processed 6000 total images\n",
      "[INFO] processed 7000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 9000 total images\n",
      "[INFO] processed 10000 total images\n",
      "[INFO] processed 11000 total images\n",
      "[INFO] processed 12000 total images\n"
     ]
    }
   ],
   "source": [
    "trainPath=os.path.sep.join([args['dataset'],'Train.csv'])\n",
    "testPath=os.path.sep.join([args['dataset'],'Test.csv'])\n",
    "\n",
    "(trainX,trainY)=load_split(args['dataset'],trainPath)\n",
    "(testX,testY)=load_split(args['dataset'],testPath)\n",
    "\n",
    "trainX=trainX.astype('float32')/255.0\n",
    "testX=testX.astype('float32')/255.0\n",
    "\n",
    "numLabels=len(np.unique(trainY))\n",
    "trainY=to_categorical(trainY,numLabels)\n",
    "testY=to_categorical(testY,numLabels)\n",
    "\n",
    "classTotals=trainY.sum(axis=0)\n",
    "classWeight=dict()\n",
    "\n",
    "for i in range(len(classTotals)):\n",
    "    classWeight[i]=classTotals.max()/classTotals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 612 steps, validate on 12630 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000022BFFA953A8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000022BFFA953A8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "612/612 [==============================] - 87s 142ms/step - loss: 5.2103 - accuracy: 0.0244 - val_loss: 10.4523 - val_accuracy: 0.0217\n",
      "Epoch 2/20\n",
      "612/612 [==============================] - 80s 131ms/step - loss: 5.2083 - accuracy: 0.0254 - val_loss: 10.2381 - val_accuracy: 0.0345\n",
      "Epoch 3/20\n",
      "612/612 [==============================] - 86s 141ms/step - loss: 5.1889 - accuracy: 0.0264 - val_loss: 10.2544 - val_accuracy: 0.0352\n",
      "Epoch 4/20\n",
      "612/612 [==============================] - 83s 136ms/step - loss: 5.2038 - accuracy: 0.0259 - val_loss: 10.2572 - val_accuracy: 0.0356\n",
      "Epoch 5/20\n",
      "612/612 [==============================] - 79s 130ms/step - loss: 5.2128 - accuracy: 0.0246 - val_loss: 10.2530 - val_accuracy: 0.0353\n",
      "Epoch 6/20\n",
      "612/612 [==============================] - 79s 129ms/step - loss: 5.2072 - accuracy: 0.0252 - val_loss: 10.2483 - val_accuracy: 0.0350\n",
      "Epoch 7/20\n",
      "612/612 [==============================] - 78s 127ms/step - loss: 5.2058 - accuracy: 0.0261 - val_loss: 10.2594 - val_accuracy: 0.0352\n",
      "Epoch 8/20\n",
      "612/612 [==============================] - 77s 126ms/step - loss: 5.2254 - accuracy: 0.0233 - val_loss: 10.2615 - val_accuracy: 0.0348\n",
      "Epoch 9/20\n",
      "612/612 [==============================] - 78s 127ms/step - loss: 5.2026 - accuracy: 0.0246 - val_loss: 10.2591 - val_accuracy: 0.0352\n",
      "Epoch 10/20\n",
      "612/612 [==============================] - 78s 128ms/step - loss: 5.2118 - accuracy: 0.0236 - val_loss: 10.2475 - val_accuracy: 0.0353\n",
      "Epoch 11/20\n",
      "612/612 [==============================] - 78s 127ms/step - loss: 5.2037 - accuracy: 0.0259 - val_loss: 10.2409 - val_accuracy: 0.0349\n",
      "Epoch 12/20\n",
      "612/612 [==============================] - 78s 128ms/step - loss: 5.2125 - accuracy: 0.0241 - val_loss: 10.2616 - val_accuracy: 0.0354\n",
      "Epoch 13/20\n",
      "612/612 [==============================] - 80s 131ms/step - loss: 5.2134 - accuracy: 0.0240 - val_loss: 10.2412 - val_accuracy: 0.0347\n",
      "Epoch 14/20\n",
      "612/612 [==============================] - 78s 127ms/step - loss: 5.2058 - accuracy: 0.0243 - val_loss: 10.2414 - val_accuracy: 0.0352\n",
      "Epoch 15/20\n",
      "612/612 [==============================] - 80s 130ms/step - loss: 5.2096 - accuracy: 0.0255 - val_loss: 10.2389 - val_accuracy: 0.0351\n",
      "Epoch 16/20\n",
      "612/612 [==============================] - 79s 130ms/step - loss: 5.2126 - accuracy: 0.0253 - val_loss: 10.2528 - val_accuracy: 0.0355\n",
      "Epoch 17/20\n",
      "612/612 [==============================] - 78s 128ms/step - loss: 5.2223 - accuracy: 0.0247 - val_loss: 10.2488 - val_accuracy: 0.0350\n",
      "Epoch 18/20\n",
      "612/612 [==============================] - 83s 135ms/step - loss: 5.2112 - accuracy: 0.0252 - val_loss: 10.2470 - val_accuracy: 0.0351\n",
      "Epoch 19/20\n",
      "612/612 [==============================] - 81s 133ms/step - loss: 5.2137 - accuracy: 0.0247 - val_loss: 10.2626 - val_accuracy: 0.0353\n",
      "Epoch 20/20\n",
      "612/612 [==============================] - 79s 129ms/step - loss: 5.2216 - accuracy: 0.0247 - val_loss: 10.2489 - val_accuracy: 0.0359\n"
     ]
    }
   ],
   "source": [
    "aug=ImageDataGenerator(rotation_range=10,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=False,\n",
    "\tvertical_flip=False,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "Num_epochs=20\n",
    "lr=1e-3\n",
    "bs=64\n",
    "\n",
    "opt=Adam(lr=lr,decay=Num_epochs/lr)\n",
    "model=TrafficSignNet.build(width=32,height=32,depth=3,classes=numLabels)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "H=model.fit(aug.flow(trainX,trainY,batch_size=bs),\n",
    "            validation_data=(testX,testY),\n",
    "            steps_per_epoch=trainX.shape[0]//bs,\n",
    "            epochs=Num_epochs,\n",
    "            class_weight=classWeight,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          27       0.00      0.00      0.00        60\n",
      "           0       0.12      0.18      0.14       720\n",
      "           1       0.06      0.01      0.02       750\n",
      "          10       0.03      0.01      0.02       450\n",
      "          11       0.06      0.05      0.06       660\n",
      "          12       0.05      0.03      0.04       630\n",
      "          13       0.00      0.00      0.00       150\n",
      "          14       0.04      0.03      0.04       450\n",
      "          15       0.04      0.01      0.01       450\n",
      "          16       0.06      0.04      0.05       480\n",
      "          17       0.04      0.06      0.05       660\n",
      "          18       0.04      0.05      0.04       420\n",
      "          19       0.04      0.01      0.01       690\n",
      "           2       0.06      0.02      0.03       720\n",
      "          20       0.03      0.02      0.02       270\n",
      "          21       0.04      0.05      0.04       210\n",
      "          22       0.00      0.00      0.00       150\n",
      "          23       0.39      0.07      0.13       360\n",
      "          24       0.02      0.04      0.03       390\n",
      "          25       0.03      0.13      0.04        60\n",
      "          26       0.01      0.02      0.01        90\n",
      "          28       0.04      0.10      0.05        90\n",
      "          29       0.01      0.02      0.01       120\n",
      "           3       0.00      0.01      0.01       150\n",
      "          30       0.00      0.00      0.00        90\n",
      "          31       0.04      0.02      0.02       480\n",
      "          32       0.00      0.00      0.00       180\n",
      "          33       0.00      0.00      0.00        60\n",
      "          34       0.02      0.04      0.03       150\n",
      "          35       0.00      0.01      0.01        90\n",
      "          36       0.00      0.00      0.00       150\n",
      "          37       0.01      0.01      0.01       270\n",
      "          38       0.00      0.00      0.00        60\n",
      "          39       0.01      0.00      0.01       210\n",
      "           4       0.00      0.01      0.00       120\n",
      "          40       0.00      0.00      0.00       390\n",
      "          41       0.02      0.04      0.03       120\n",
      "          42       0.00      0.02      0.01        60\n",
      "           5       0.06      0.01      0.02       690\n",
      "           6       0.00      0.00      0.00        90\n",
      "           7       0.00      0.00      0.00        90\n",
      "           8       0.08      0.12      0.09        60\n",
      "           9       0.03      0.12      0.05        90\n",
      "\n",
      "    accuracy                           0.04     12630\n",
      "   macro avg       0.03      0.03      0.03     12630\n",
      "weighted avg       0.05      0.04      0.04     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "predictions=model.predict(testX,batch_size=bs)\n",
    "print(classification_report(testY.argmax(axis=1),predictions.argmax(axis=1),target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('TrafficSignClassification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "my_model=load_model('TrafficSignClassification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 8)         608       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 8)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 16)        1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 43)                11051     \n",
      "=================================================================\n",
      "Total params: 129,675\n",
      "Trainable params: 128,699\n",
      "Non-trainable params: 976\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'test_set':r\"F:\\Computer Vision Datasets\\Traffic Sign Detection\\archive (7)\\Test\",\n",
    "     'output_directory':r\"F:\\Computer Vision Datasets\\Traffic Sign Detection\\Testing Directory\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelNames=open('Meta.csv').read().strip().split(\"\\n\")[1:]\n",
    "labelNames=[l.split(\",\")[1] for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths=list(paths.list_images(args['test_set']))\n",
    "random.shuffle(imagePaths)\n",
    "imagePaths=imagePaths[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,imagePath) in enumerate(imagePaths):\n",
    "    image=io.imread(imagePath)\n",
    "    image=transform.resize(image,(32,32))\n",
    "    image=exposure.equalize_adapthist(image,clip_limit=0.1)\n",
    "    \n",
    "    image=image.astype('float32')/255.0\n",
    "    image=np.expand_dims(image,axis=0)\n",
    "    \n",
    "    preds=my_model.predict(image)\n",
    "    j=preds.argmax(axis=1)[0]\n",
    "    label=labelNames[j]\n",
    "    \n",
    "    image=cv2.imread(imagePath)\n",
    "    image=imutils.resize(image,width=128)\n",
    "    cv2.putText(image,label,(5,15),cv2.FONT_HERSHEY_SIMPLEX,0.25,(0,0,255),2)\n",
    "    \n",
    "    p=os.path.sep.join([args['output_directory'],\"{}.png\".format(i)])\n",
    "    cv2.imwrite(p,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
