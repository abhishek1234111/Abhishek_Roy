{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import keras\n",
    "import imutils\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,BatchNormalization,Dropout,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training our OCR Model using Keras and TensorFlow\n",
    "\n",
    "class ResNet:\n",
    "    def build(width,height,depth,classes):\n",
    "        inputshape=(height,width,depth)\n",
    "        model=Sequential()\n",
    "        model.add(Conv2D(32,(5,5),activation='relu',padding='same',input_shape=inputshape))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(64,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(classes,activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for loading the A-Z and MNIST datasets, respectively\n",
    "def load_az_dataset(datasetPath):\n",
    "    data=[]\n",
    "    label=[]\n",
    "    for row in open(datasetPath):\n",
    "        row=row.split(',')\n",
    "        labels=int(row[0])\n",
    "        image=np.array([int(x) for x in row[1:]],dtype='uint8')\n",
    "        \n",
    "        image=image.reshape((28,28))\n",
    "        \n",
    "        data.append(image)\n",
    "        label.append(labels)\n",
    "        \n",
    "    data=np.array(data,dtype='float32')\n",
    "    label=np.array(label,dtype='int')\n",
    "    \n",
    "    return (data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def load_mnist_dataset():\n",
    "    ((trainData,trainLabels),(testData,testLabels))=mnist.load_data()\n",
    "    \n",
    "    data=np.vstack([trainData,testData])\n",
    "    labels=np.hstack([trainLabels,testLabels])\n",
    "    \n",
    "    return (data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'a-z_dataset':r\"F:\\Computer Vision 2\\HandWriting Recognition\\archive (4)\\A_Z Handwritten Data\\A_Z Handwritten Data.csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_epochs=50\n",
    "lr=1e-1\n",
    "bs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading databases.....\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] loading databases.....')\n",
    "(azData,azLabels)=load_az_dataset(args['a-z_dataset'])\n",
    "(digitsData,digitsLabels)=load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the MNIST dataset occupies the labels 0-9, so let's add 10 to every\n",
    "# A-Z label to ensure the A-Z characters are not incorrectly labeled\n",
    "# as digits\n",
    "azLabels+=10\n",
    "\n",
    "data=np.vstack([azData,digitsData])\n",
    "labels=np.hstack([azLabels,digitsLabels])\n",
    "\n",
    "data=[cv2.resize(image,(32,32)) for image in data]\n",
    "data=np.array(data,dtype='float32')\n",
    "\n",
    "data=np.expand_dims(data,axis=-1)\n",
    "data/=255.0\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb=LabelBinarizer()\n",
    "labels=lb.fit_transform(labels)\n",
    "counts=labels.sum(axis=0)\n",
    "\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals=labels.sum(axis=0)\n",
    "classWeight={}\n",
    "\n",
    "for i in range(0,len(classTotals)):\n",
    "    classWeight[i]=classTotals.max()/classTotals[i]\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "(trainX,testX,trainY,testY)=train_test_split(data,labels,test_size=0.20,stratify=labels,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug=ImageDataGenerator(rotation_range=10,\n",
    "    zoom_range=0.05,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=lr)\n",
    "model=ResNet.build(width=32,height=32,depth=1,classes=len(lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 36)                2340      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 366,820\n",
      "Trainable params: 366,436\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001C4A93E14C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001C4A93E14C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2765/2765 [==============================] - ETA: 0s - loss: 6.9125 - accuracy: 0.5348WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001C4ABB1E8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001C4ABB1E8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2765/2765 [==============================] - 468s 167ms/step - loss: 6.9125 - accuracy: 0.5348 - val_loss: 1.3264 - val_accuracy: 0.6234\n",
      "Epoch 2/2\n",
      "2765/2765 [==============================] - 465s 167ms/step - loss: 4.4119 - accuracy: 0.7006 - val_loss: 0.9887 - val_accuracy: 0.7537\n"
     ]
    }
   ],
   "source": [
    "H=model.fit(aug.flow(trainX,trainY,batch_size=bs),\n",
    "            validation_data=(testX,testY),\n",
    "            steps_per_epoch=len(trainX)//bs,\n",
    "             epochs=2,\n",
    "             class_weight=classWeight,\n",
    "             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelNames='0123456789'\n",
    "labelNames+='ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "labelNames=[l for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001C4AA5E4798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001C4AA5E4798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.57      0.16      1381\n",
      "           1       0.99      0.91      0.95      1575\n",
      "           2       0.67      0.97      0.79      1398\n",
      "           3       0.84      0.96      0.89      1428\n",
      "           4       0.81      0.92      0.86      1365\n",
      "           5       0.22      0.92      0.36      1263\n",
      "           6       0.97      0.87      0.91      1375\n",
      "           7       0.76      0.95      0.84      1459\n",
      "           8       0.57      0.98      0.72      1365\n",
      "           9       0.84      0.98      0.91      1392\n",
      "           A       0.99      0.92      0.95      2774\n",
      "           B       0.98      0.87      0.92      1734\n",
      "           C       1.00      0.86      0.92      4682\n",
      "           D       0.64      0.95      0.76      2027\n",
      "           E       0.95      0.95      0.95      2288\n",
      "           F       0.97      0.96      0.96       232\n",
      "           G       0.80      0.87      0.83      1152\n",
      "           H       0.96      0.89      0.92      1444\n",
      "           I       0.93      0.98      0.95       224\n",
      "           J       0.91      0.96      0.93      1699\n",
      "           K       0.99      0.83      0.90      1121\n",
      "           L       0.94      0.96      0.95      2317\n",
      "           M       0.82      0.99      0.90      2467\n",
      "           N       1.00      0.79      0.88      3802\n",
      "           O       0.94      0.07      0.13     11565\n",
      "           P       1.00      0.90      0.95      3868\n",
      "           Q       0.25      0.97      0.39      1162\n",
      "           R       0.97      0.94      0.95      2313\n",
      "           S       0.99      0.50      0.66      9684\n",
      "           T       1.00      0.89      0.94      4499\n",
      "           U       0.97      0.93      0.95      5802\n",
      "           V       1.00      0.95      0.98       836\n",
      "           W       0.77      0.98      0.86      2157\n",
      "           X       0.85      0.99      0.91      1254\n",
      "           Y       0.94      0.91      0.92      2172\n",
      "           Z       0.92      0.80      0.85      1215\n",
      "\n",
      "    accuracy                           0.75     88491\n",
      "   macro avg       0.84      0.88      0.82     88491\n",
      "weighted avg       0.89      0.75      0.75     88491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from numpy import argmax\n",
    "preds=model.predict(testX,batch_size=bs)\n",
    "print(classification_report(testY.argmax(axis=1),preds.argmax(axis=1),target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for our visualization procedure so we can see whether our model is working or not\n",
    "\n",
    "from imutils import build_montages\n",
    "images=[]\n",
    "\n",
    "for i in np.random.choice(np.arange(0,len(testY)),size=(49,)):\n",
    "    probs=model.predict(testX[np.newaxis,i])\n",
    "    prediction=probs.argmax(axis=1)\n",
    "    label=labelNames[prediction[0]]\n",
    "    \n",
    "    image=(testX[i]*255).astype('uint8')\n",
    "    color=(0,255,0)\n",
    "    \n",
    "    if prediction[0]!=np.argmax(testY[i]):\n",
    "        color=(0,0,255)\n",
    "        \n",
    "    # merge the channels into one image, resize the image from 32x32\n",
    "    # to 96x96 so we can better see it and then draw the predicted\n",
    "    # label on the image\n",
    "    image=cv2.merge([image]*3)\n",
    "    image=cv2.resize(image,(96,96),interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.putText(image,label,(5,20),cv2.FONT_HERSHEY_SIMPLEX,0.75,color,2)\n",
    "    \n",
    "    images.append(image)\n",
    "    \n",
    "montage=build_montages(images,(96,96),(7,7))[0]\n",
    "\n",
    "cv2.imshow('Montages',montage)\n",
    "cv2.waitKey(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save('Handwritten_recog_26042023.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from imutils.contours import sort_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "my_model=load_model('Handwritten_recog_26042023.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'input_image':r\"F:\\Dog images\\0Jl54.png\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]2-100.00%\n",
      "[INFO]Q-73.84%\n",
      "[INFO]Q-84.18%\n",
      "[INFO]E-56.29%\n",
      "[INFO]2-84.87%\n",
      "[INFO]2-68.53%\n",
      "[INFO]E-94.37%\n",
      "[INFO]8-100.00%\n",
      "[INFO]E-100.00%\n",
      "[INFO]E-100.00%\n",
      "[INFO]S-100.00%\n",
      "[INFO]D-95.71%\n",
      "[INFO]M-78.02%\n",
      "[INFO]E-97.91%\n",
      "[INFO]D-93.50%\n",
      "[INFO]W-100.00%\n",
      "[INFO]W-100.00%\n",
      "[INFO]E-90.55%\n",
      "[INFO]8-83.32%\n",
      "[INFO]Q-81.43%\n",
      "[INFO]E-99.78%\n",
      "[INFO]2-85.20%\n",
      "[INFO]2-99.78%\n",
      "[INFO]Q-99.16%\n",
      "[INFO]2-100.00%\n",
      "[INFO]D-94.05%\n",
      "[INFO]2-89.36%\n",
      "[INFO]5-71.82%\n",
      "[INFO]5-60.42%\n",
      "[INFO]5-54.09%\n",
      "[INFO]E-99.96%\n",
      "[INFO]2-98.68%\n",
      "[INFO]Q-99.93%\n",
      "[INFO]E-99.92%\n",
      "[INFO]E-97.44%\n",
      "[INFO]E-87.45%\n",
      "[INFO]S-100.00%\n",
      "[INFO]2-57.61%\n",
      "[INFO]Q-97.51%\n",
      "[INFO]S-68.34%\n",
      "[INFO]E-94.19%\n",
      "[INFO]Q-99.70%\n",
      "[INFO]8-97.42%\n",
      "[INFO]2-66.03%\n",
      "[INFO]D-97.27%\n",
      "[INFO]Q-99.55%\n",
      "[INFO]W-100.00%\n",
      "[INFO]E-99.16%\n",
      "[INFO]2-75.02%\n",
      "[INFO]2-89.60%\n",
      "[INFO]I-50.48%\n",
      "[INFO]2-90.58%\n",
      "[INFO]B-88.14%\n",
      "[INFO]2-100.00%\n",
      "[INFO]2-85.08%\n",
      "[INFO]5-61.25%\n",
      "[INFO]W-100.00%\n",
      "[INFO]2-90.64%\n",
      "[INFO]5-53.24%\n",
      "[INFO]E-98.14%\n",
      "[INFO]8-97.65%\n",
      "[INFO]I-97.50%\n",
      "[INFO]5-61.61%\n",
      "[INFO]E-93.80%\n",
      "[INFO]2-100.00%\n",
      "[INFO]2-86.73%\n",
      "[INFO]W-100.00%\n",
      "[INFO]E-58.12%\n",
      "[INFO]8-97.12%\n",
      "[INFO]5-82.89%\n",
      "[INFO]E-98.64%\n",
      "[INFO]E-99.81%\n",
      "[INFO]Q-65.62%\n",
      "[INFO]2-100.00%\n",
      "[INFO]E-80.96%\n",
      "[INFO]S-54.10%\n",
      "[INFO]5-51.26%\n",
      "[INFO]8-89.28%\n",
      "[INFO]E-99.73%\n",
      "[INFO]S-54.55%\n",
      "[INFO]W-100.00%\n",
      "[INFO]E-99.77%\n",
      "[INFO]I-78.58%\n",
      "[INFO]Q-89.08%\n",
      "[INFO]8-89.46%\n",
      "[INFO]2-100.00%\n",
      "[INFO]2-57.87%\n",
      "[INFO]8-78.59%\n",
      "[INFO]E-98.86%\n",
      "[INFO]8-100.00%\n",
      "[INFO]5-60.17%\n",
      "[INFO]E-93.67%\n",
      "[INFO]E-99.88%\n"
     ]
    }
   ],
   "source": [
    "image=cv2.imread(args['input_image'])\n",
    "gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "blurred=cv2.GaussianBlur(gray,(5,5),0)\n",
    "\n",
    "\n",
    "# perform edge detection, find contours in the edge map, and sort the\n",
    "# resulting contours from left-to-right\n",
    "edged=cv2.Canny(blurred,30,150)\n",
    "cnts=cv2.findContours(edged.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts=imutils.grab_contours(cnts)\n",
    "cnts=sort_contours(cnts,method='left-to-right')[0]\n",
    "\n",
    "chars=[]\n",
    "\n",
    "for c in cnts:\n",
    "    (x,y,w,h)=cv2.boundingRect(c)\n",
    "    if (w>=5 and w<=150) and (h>=15 and h<=120):\n",
    "        roi=gray[y:y+h,x:x+w]\n",
    "        thresh=cv2.threshold(roi,0,255,cv2.THRESH_BINARY_INV|cv2.THRESH_OTSU)[1]\n",
    "        (tW,tH)=thresh.shape\n",
    "        \n",
    "        if tW>tH:\n",
    "            thresh=imutils.resize(thresh,width=32)\n",
    "        else:\n",
    "            thresh=imutils.resize(thresh,height=32)\n",
    "            \n",
    "        # re-grab the image dimensions (now that its been resized)\n",
    "        # and then determine how much we need to pad the width and\n",
    "        # height such that our image will be 32x32\n",
    "        \n",
    "        (tW,tH)=thresh.shape\n",
    "        dx=int(max(0,32-tW)/2.0)\n",
    "        dy=int(max(0,32-tH)/2.0)\n",
    "        \n",
    "        padded=cv2.copyMakeBorder(thresh,top=dy,bottom=dx,left=dx,right=dx,borderType=cv2.BORDER_CONSTANT,value=(0,0,0))\n",
    "        \n",
    "        \n",
    "        # pad the image and force 32x32 dimensions\n",
    "        padded=cv2.resize(padded,(32,32))\n",
    "        padded=padded.astype('float32')/255.0\n",
    "        padded=np.expand_dims(padded,axis=-1)\n",
    "        \n",
    "        chars.append((padded,(x,y,w,h)))\n",
    "        \n",
    "        \n",
    "boxes=[b[1] for b in chars]\n",
    "chars=np.array([c[0] for c in chars],dtype='float32')\n",
    "\n",
    "#Applying our OCR model to handwriting recognition\n",
    "preds=my_model.predict(chars)\n",
    "\n",
    "labelNames='0123456789'\n",
    "labelNames+='ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "labelNames=[l for l in labelNames]     \n",
    "\n",
    "for (pred,(x,y,w,h)) in zip(preds,boxes):\n",
    "    i=np.argmax(pred)\n",
    "    probs=pred[i]\n",
    "    label=labelNames[i]\n",
    "    \n",
    "    print(\"[INFO]{}-{:.2f}%\".format(label,probs*100))\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    cv2.putText(image,label,(x-10,y-10),cv2.FONT_HERSHEY_SIMPLEX,0.75,(0,255,0),2)\n",
    "    \n",
    "    cv2.imshow('Image',image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
